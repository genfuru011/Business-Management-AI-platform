#!/usr/bin/env node

/**
 * Ollama Proxy Server with API Key Authentication
 * OpenAI APIäº’æ›ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’æä¾›ã—ã€å†…éƒ¨ã®Ollamaã‚µãƒ¼ãƒãƒ¼ã«ãƒ—ãƒ­ã‚­ã‚·ã—ã¾ã™
 */

const express = require('express');
const { createProxyMiddleware } = require('http-proxy-middleware');
const cors = require('cors');
const rateLimit = require('express-rate-limit');

const app = express();
const PORT = process.env.OLLAMA_PROXY_PORT || 11435;
const OLLAMA_HOST = process.env.OLLAMA_HOST || 'http://localhost:11434';

// ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢è¨­å®š
app.use(express.json());
app.use(cors());

// ãƒ¬ãƒ¼ãƒˆåˆ¶é™
const limiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15åˆ†
  max: 100, // æœ€å¤§100ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
  message: { error: 'Too many requests from this IP' }
});
app.use(limiter);

// APIã‚­ãƒ¼èªè¨¼ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢
const apiKeyAuth = (req, res, next) => {
  const authHeader = req.headers['authorization'];
  const apiKey = authHeader?.replace('Bearer ', '');
  
  // ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®APIã‚­ãƒ¼
  const validApiKeys = (process.env.OLLAMA_API_KEYS || 'ollama-local-key-123,ollama-dev-key-456').split(',');
  
  console.log(`[${new Date().toISOString()}] API Key Request: ${apiKey ? '***' : 'None'}`);
  
  if (!apiKey || !validApiKeys.includes(apiKey)) {
    console.log(`[${new Date().toISOString()}] Unauthorized request from ${req.ip}`);
    return res.status(401).json({ 
      error: { 
        message: 'Invalid API key provided',
        type: 'invalid_request_error',
        code: 'invalid_api_key'
      }
    });
  }
  
  console.log(`[${new Date().toISOString()}] Authorized request for ${req.path}`);
  next();
};

// ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
app.get('/health', (req, res) => {
  res.json({ 
    status: 'ok', 
    timestamp: new Date().toISOString(),
    ollama_host: OLLAMA_HOST 
  });
});

// ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ (OpenAIäº’æ›)
app.get('/v1/models', apiKeyAuth, async (req, res) => {
  try {
    const response = await fetch(`${OLLAMA_HOST}/api/tags`);
    const data = await response.json();
    
    // OpenAIå½¢å¼ã«å¤‰æ›
    const models = data.models?.map(model => ({
      id: model.name,
      object: 'model',
      created: Math.floor(Date.now() / 1000),
      owned_by: 'ollama',
      permission: [],
      root: model.name,
      parent: null
    })) || [];
    
    res.json({
      object: 'list',
      data: models
    });
  } catch (error) {
    console.error('Models endpoint error:', error);
    res.status(500).json({ error: 'Failed to fetch models' });
  }
});

// ãƒãƒ£ãƒƒãƒˆè£œå®Œ (OpenAIäº’æ›)
app.post('/v1/chat/completions', apiKeyAuth, async (req, res) => {
  try {
    const { model, messages, stream = false, temperature, max_tokens, ...otherParams } = req.body;
    
    console.log(`[${new Date().toISOString()}] Chat request for model: ${model}`);
    
    // Ollamaå½¢å¼ã«å¤‰æ›
    const ollamaRequest = {
      model: model || 'llama3.2',
      messages: messages,
      stream: stream,
      options: {
        temperature: temperature,
        num_predict: max_tokens,
        ...otherParams
      }
    };
     if (stream) {
      // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), 300000); // 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
      
      const response = await fetch(`${OLLAMA_HOST}/api/chat`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(ollamaRequest),
        signal: controller.signal
      });
      
      clearTimeout(timeoutId);
      
      res.setHeader('Content-Type', 'text/plain; charset=utf-8');
      res.setHeader('Cache-Control', 'no-cache');
      res.setHeader('Connection', 'keep-alive');
      
      const reader = response.body?.getReader();
      if (!reader) throw new Error('No response body');
      
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        
        const chunk = new TextDecoder().decode(value);
        const lines = chunk.split('\n').filter(line => line.trim());
        
        for (const line of lines) {
          try {
            const ollamaData = JSON.parse(line);
            
            // OpenAIå½¢å¼ã«å¤‰æ›
            const openaiChunk = {
              id: `chatcmpl-${Date.now()}`,
              object: 'chat.completion.chunk',
              created: Math.floor(Date.now() / 1000),
              model: model,
              choices: [{
                index: 0,
                delta: {
                  content: ollamaData.message?.content || ''
                },
                finish_reason: ollamaData.done ? 'stop' : null
              }]
            };
            
            res.write(`data: ${JSON.stringify(openaiChunk)}\n\n`);

            if (ollamaData.done) {
              res.write('data: [DONE]\n\n');
              res.end();
              return;
            }
          } catch (parseError) {
            // JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
          }
        }
      }
    } else {
      // éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), 300000); // 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
      
      const response = await fetch(`${OLLAMA_HOST}/api/chat`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(ollamaRequest),
        signal: controller.signal
      });
      
      clearTimeout(timeoutId);
      
      const data = await response.json();
      
      // OpenAIå½¢å¼ã«å¤‰æ›
      const openaiResponse = {
        id: `chatcmpl-${Date.now()}`,
        object: 'chat.completion',
        created: Math.floor(Date.now() / 1000),
        model: model,
        choices: [{
          index: 0,
          message: {
            role: 'assistant',
            content: data.message?.content || ''
          },
          finish_reason: 'stop'
        }],
        usage: {
          prompt_tokens: data.prompt_eval_count || 0,
          completion_tokens: data.eval_count || 0,
          total_tokens: (data.prompt_eval_count || 0) + (data.eval_count || 0)
        }
      };
      
      res.json(openaiResponse);
    }
  } catch (error) {
    console.error('Chat completion error:', error);
    
    if (error.name === 'AbortError') {
      res.status(408).json({ 
        error: { 
          message: 'Request timeout - The AI model took too long to respond', 
          type: 'timeout_error',
          code: 'request_timeout'
        } 
      });
    } else {
      res.status(500).json({ 
        error: { 
          message: 'Internal server error', 
          type: 'server_error' 
        } 
      });
    }
  }
});

// ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
app.use((error, req, res, next) => {
  console.error('Server error:', error);
  res.status(500).json({ error: 'Internal server error' });
});

// ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
app.listen(PORT, () => {
  console.log(`ğŸš€ Ollama Proxy Server running on port ${PORT}`);
  console.log(`ğŸ“¡ Proxying to Ollama at ${OLLAMA_HOST}`);
  console.log(`ğŸ”‘ Valid API keys: ${process.env.OLLAMA_API_KEYS ? '***' : 'ollama-local-key-123,ollama-dev-key-456'}`);
  console.log(`ğŸŒ OpenAI compatible endpoints:`);
  console.log(`   - GET  http://localhost:${PORT}/v1/models`);
  console.log(`   - POST http://localhost:${PORT}/v1/chat/completions`);
  console.log(`   - GET  http://localhost:${PORT}/health`);
});

module.exports = app;
