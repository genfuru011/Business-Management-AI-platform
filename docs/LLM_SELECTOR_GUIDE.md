# 🚀 LLMセレクト機能ガイド

## 概要

ビジネス管理AIプラットフォームにLLMセレクト機能が追加されました。これにより、ユーザーは以下の様々なAIプロバイダーを選択して使用できるようになります：

### 対応プロバイダー

| プロバイダー | モデル例 | 特徴 | データ要件 |
|------------|----------|------|-----------|
| **OpenAI** | GPT-4o, GPT-4o Mini, GPT-4 Turbo | 最新の高性能AI | APIキー必要 |
| **Anthropic Claude** | Claude 3.5 Sonnet, Claude 3 Haiku, Claude 3 Opus | 安全性・倫理性重視 | APIキー必要 |
| **Google Gemini** | Gemini 1.5 Pro, Gemini 1.5 Flash | マルチモーダル対応 | APIキー必要 |
| **Ollama (ローカル)** | Llama 3.2, Llama 3.1, TinyLlama, Code Llama | 完全ローカル実行 | APIキー不要 |

## 🎯 使用方法

### 1. LLMプロバイダーの選択

1. ダッシュボードページでAIエージェントパネルを展開
2. 設定アイコン（⚙️）をクリック
3. 使用したいプロバイダーを選択
4. 対応するモデルを選択

### 2. APIキーの設定

クラウドプロバイダー（OpenAI、Claude、Gemini）を使用する場合：

1. 各プロバイダーの公式サイトでAPIキーを取得
2. LLM設定画面の「APIキー」フィールドに入力
3. 「保存」をクリック

### 3. カスタムエンドポイント（オプション）

プロキシサーバーや独自エンドポイントを使用する場合：

1. 「カスタムエンドポイント」フィールドに入力
2. 例：`https://your-proxy.com/v1`

## 🔒 セキュリティ機能

### データ保護
- **ローカルモデル**: データは外部に送信されません
- **クラウドモデル**: 各プロバイダーのプライバシーポリシーに従います
- **APIキー**: ブラウザのlocalStorageに暗号化されて保存

### 設定の永続化
- 選択されたプロバイダー・モデル・APIキーはブラウザに保存
- ページを再読み込みしても設定は維持されます
- 複数のブラウザ/デバイスでは個別に設定が必要

## 📊 推奨設定

### データセキュリティ重視
```
プロバイダー: Ollama (ローカル)
モデル: Llama 3.2
特徴: 完全にローカル実行、データが外部に送信されない
```

### 高性能重視
```
プロバイダー: OpenAI
モデル: GPT-4o
特徴: 最新の高性能AI、優れた日本語対応
```

### コスト効率重視
```
プロバイダー: OpenAI
モデル: GPT-4o Mini
特徴: 高速・低コスト、十分な性能
```

### 安全性重視
```
プロバイダー: Anthropic Claude
モデル: Claude 3.5 Sonnet
特徴: 安全性・倫理性を重視した設計
```

## 🛠️ トラブルシューティング

### よくある問題

#### 1. APIキーエラー
- **症状**: 「API key invalid」エラー
- **解決**: 正しいAPIキーが入力されているか確認
- **確認方法**: プロバイダーの管理画面でAPIキーの有効性をチェック

#### 2. ローカルモデルが利用できない
- **症状**: Ollamaプロバイダーが応答しない
- **解決**: Ollamaサービスが起動しているか確認
- **確認方法**: `ollama serve` でサービス起動

#### 3. モデルが見つからない
- **症状**: 「Model not found」エラー
- **解決**: 選択したモデルがプロバイダーで利用可能か確認
- **Ollama**: `ollama pull llama3.2` でモデルをダウンロード

### 設定のリセット

設定に問題がある場合は、ブラウザの開発者ツールで以下を実行：

```javascript
localStorage.removeItem('business-ai-llm-settings')
location.reload()
```

## 🔄 後方互換性

### 環境変数での設定（非推奨）

以前の環境変数による設定も引き続き動作しますが、UI設定が優先されます：

```bash
# レガシー設定（フォールバック用）
NEXT_PUBLIC_AI_PROVIDER=ollama
NEXT_PUBLIC_AI_MODEL=llama3.2
NEXT_PUBLIC_AI_API_KEY=your-api-key
NEXT_PUBLIC_AI_ENDPOINT=http://localhost:11434/v1
```

## 📈 今後のアップデート予定

- [ ] より多くのプロバイダーのサポート
- [ ] モデル性能の比較機能
- [ ] 使用量の追跡・分析
- [ ] チーム設定の共有機能

---

**作成日**: 2025年6月8日  
**バージョン**: 1.0.0  
**更新履歴**: 初回リリース